{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload for packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import ast\n",
    "\n",
    "# Import viz tools\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modelling packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Logger\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logger\n",
    "# Load config and logger\n",
    "from eztools.operations import Logger, ConfigReader\n",
    "logger = Logger('/mnt/logs/', logger_name = 'L&L').get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from src.etl.get_data import read_csv_data\n",
    "from src.etl.get_missing_values import get_df_na, get_na_columns, impute_nan, plot_kdensity\n",
    "from src.etl.get_train_test_set import get_train_test_set\n",
    "from src.etl.mlops import string_to_list\n",
    "from src.ml.get_lasso_model_predictions import get_lasso_model_predictions\n",
    "from src.ml.get_model_accuracy import get_model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config.ini\n",
    "CONFIG_PATH = '/repos/poc-model-drift/src/config/config.ini'\n",
    "config = ConfigReader(CONFIG_PATH, config_tuple = False).read_config()\n",
    "\n",
    "# Unpack config\n",
    "DATA_PATH = config['data']['data_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = read_csv_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot info about the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Calculate missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get df with na values\n",
    "df_na = get_df_na(df)\n",
    "df_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with nan values\n",
    "COLS_TO_IMPUTE = get_na_columns(df_na)\n",
    "COLS_TO_IMPUTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Impute nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of the missing columns\n",
    "plot_kdensity(df, 'pH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute nan values\n",
    "df = impute_nan(df, cols = 'pH', replacement = 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Lasso Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = get_train_test_set(df, response = 'wine_colour', pos_class = 'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train lasso model & make the predictions\n",
    "y_pred = get_lasso_model_predictions(X_train, X_test, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification metrics\n",
    "accuracy = get_model_accuracy(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Drift in Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model_monitor packages\n",
    "from src.ml.model_monitor import ModelMonitorReports, MonitorReportReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the config file\n",
    "LATEST_DRIFT_REPORT = config['json_reports_path']['latest_drift_report_json']\n",
    "DRIFT_EMAIL_RECEIVER = string_to_list(config['settings']['drift_email_receiver'])\n",
    "PROJECT_NAME = config['settings']['project_name']\n",
    "\n",
    "MODEL_PATH = config['model_monitor']['model_path']\n",
    "REFERENCE_DATA_PATH = config['model_monitor']['reference_data_path']\n",
    "MODEL_PATH = config['model_monitor']['model_path']\n",
    "\n",
    "# Take today's data to capture the latest report\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "LATEST_DRIFT_REPORT_PATH = LATEST_DRIFT_REPORT.format(today=today)\n",
    "\n",
    "# Have the column mapping as a global parameter\n",
    "column_mapping = ast.literal_eval(config['model_monitor']['column_mapping'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _\"evidently\"_ package allows of the following model monitor options: <br><br>\n",
    "_**1. Data Drift**_ <br>\n",
    "    Data drift is checking to see if there is any drift between the **independent** variables of the reference & current data. <br><br>\n",
    "_**2. Target and/or Prediction Drift (a.k.a. Model drift)**_ <br>\n",
    "Target and/or prediction drift detects if drift occures in the **response** (ground truth) and/or **predictions**. <br><br>\n",
    "_**3. Performance Monitor**_ <br>\n",
    "Performance monitor can check for overall **performance** of models for different **metrics** and different segments of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As part of the predictions in production, we are expected to use the pre-trained model\n",
    "lasso_model = joblib.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the reference data (with target and predict)\n",
    "X_train['prediction'] = lasso_model.predict(X_train)\n",
    "X_train['prediction'] = X_train['prediction'].map({1: 'white', 0: 'red'})\n",
    "\n",
    "df_reference = pd.concat([X_train, y_train], axis = 1).reset_index(drop=True)\n",
    "df_reference['wine_colour'] = df_reference['wine_colour'].map({1: 'white', 0: 'red'})\n",
    "df_reference.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the current data (with target and predict)\n",
    "X_test['prediction'] = lasso_model.predict(X_test)\n",
    "X_test['prediction'] = X_test['prediction'].map({1: 'white', 0: 'red'})\n",
    "\n",
    "df_current = pd.concat([X_test, y_test], axis = 1).reset_index(drop=True)\n",
    "df_current['wine_colour'] = df_current['wine_colour'].map({1: 'white', 0: 'red'})\n",
    "df_current.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Column Mapping**_ <br>\n",
    "Column Mapping is a dictionary that holds information about the data at hand. More specifically, it is a dictionary that shows:\n",
    "1. Column name of the _response_\n",
    "2. Column names of the _numerical_ independent variables\n",
    "3. Column names of the _categorical_ independent variables <br>\n",
    "These should already be encoded (e.g. one-hot encoded)\n",
    "\n",
    "For more information, please refer to the \"boston_example.ipynb\" in the \"/src/notebooks/\" location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the literal evaluation\n",
    "column_mapping = ast.literal_eval(config['model_monitor']['column_mapping'])\n",
    "column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model monitor reports by using the ModelMonitorReports\n",
    "# Note: The ModelMonitorReports could be imported from ezTools\n",
    "model_monitor_reports = ModelMonitorReports(df_reference, df_current, column_mapping)\n",
    "model_monitor_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the model performance report\n",
    "model_monitor_reports.generate_model_performance_report(report_type = 'classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Usage of the model performance report**_\n",
    "1. **Analyse the results of a model test:** <br>\n",
    "Explore the results of an offline test and contrast the different model's performance in an A/B test or shadow model deployment <br>\n",
    "(possible integration with MLFlow in the future)\n",
    "2. **To analyze the model performance on the slices of data** <br>\n",
    "By manipulating the input data frame, you can explore how the model performs on different data segments (e.g. users from a specific region).\n",
    "3. **To trigger or decide on the model retraining** <br>\n",
    "You can use this report to check if your performance is below the threshold to initiate a model update and evaluate if retraining is likely to improve performance.\n",
    "4. **To debug or improve model performance.** <br>\n",
    "You can use the Classification Quality table to identify underperforming segments and decide on ways to address them.\n",
    "\n",
    "For more information about the performance reports please address to the following link: <br>\n",
    "https://docs.evidentlyai.com/reports/classification-performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO\n",
    "\n",
    "Create the automation part of triggering an alert if any of the metrics degrade below to a pre-specified threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea: Generate a pd.DataFrame that holds the following information:\n",
    "# ____________________________________________________________\n",
    "#\n",
    "#   Metrics  |    Decay (Ref - Curent)    |  Decay Threshold |  Flag\n",
    "# --------------------------------------------------------------\n",
    "#  Accuracy   |      2%                    |  3%             |  False\n",
    "# Sensitivity |      5%                    |  3%             |  True\n",
    "#    ...      |     ...                    |  ...            |  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Production - Day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As part of the predictions in production, we are expected to use the pre-trained model\n",
    "lasso_model = joblib.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the reference data (i.e. data used for training)\n",
    "df_reference = df.copy()\n",
    "\n",
    "# Get the production data (i.e. new data that our model is expected to classify in production)\n",
    "# Note: Data has been stored by Ioannis M. for demonstrating purposes\n",
    "df_production_day1 = pd.read_pickle('src/data/assets/df_day1.pickle')\n",
    "df_production_day1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction on the training data (just for demonstrating purposes)\n",
    "df_production_day1['prediction'] = lasso_model.predict(df_production_day1)\n",
    "df_production_day1['prediction'] = df_production_day1['prediction'].map({1: 'white', 0: 'red'})\n",
    "df_production_day1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Target as it is not needed when the ground truth is not available\n",
    "df_reference.drop('wine_colour', axis=1, inplace=True)\n",
    "\n",
    "# Make the prediction on the training data (just for demonstrating purposes)\n",
    "df_reference['prediction'] = lasso_model.predict(df_reference)\n",
    "df_reference['prediction'] = df_reference['prediction'].map({1: 'white', 0: 'red'})\n",
    "\n",
    "# Explore how the reference data looks\n",
    "df_reference.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the column mapping dictionary\n",
    "column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the column mapping\n",
    "column_mapping['target'] = None\n",
    "model_monitor_reports.update_column_mapping(column_mapping)\n",
    "\n",
    "# Check the updated column mapping\n",
    "model_monitor_reports.column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the model & prediction drift in production (e.g. day 1)\n",
    "model_monitor_reports.generate_model_data_drift_report(response_type = 'categorical', report_name = 'poc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Usage of the data and/or model drift reports**_\n",
    "1. **Support model maintenance** <br>\n",
    "Decide on when to retrain the model or which features to drop due to drift.\n",
    "2. **When debugging model decay** <br>\n",
    "If the model quality has dropped, the dashboard can help explore where the change comes from.\n",
    "3. **When no ground truth is available** <br>\n",
    "You can use this report to check if your performance is below the threshold to initiate a model update and evaluate if retraining is likely to improve performance.\n",
    "4. **To debug or improve model performance.** <br>\n",
    "If there is no immediate feedback, the dashboard can be used to explore the changes in the model output and the relationship between the features and prediction.\n",
    "\n",
    "For more information about the data and model reports please address to the following links: <br>\n",
    "1. https://docs.evidentlyai.com/reports/classification-performance\n",
    "2. https://docs.evidentlyai.com/reports/data-drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MonitorReportReader class that automates the generation \n",
    "# of the automation alter in case drift is detected\n",
    "monitor_report = MonitorReportReader(LATEST_DRIFT_REPORT_PATH, PROJECT_NAME)\n",
    "monitor_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe for data drift that shows if drift is detected\n",
    "# anywhere in the independent variables\n",
    "monitor_report.create_data_drift_table()\n",
    "monitor_report.df_data_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an e-mail alert if drift is detected \"data_drift\"\n",
    "monitor_report.send_drift_email_alert(DRIFT_EMAIL_RECEIVER, send_for = 'data_drift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe for model drift that shows if drift is detected\n",
    "# anywhere in the target/response variables\n",
    "monitor_report.create_model_drift_table()\n",
    "monitor_report.df_target_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an e-mail alert if drift is detected for \"model_drift\"\n",
    "monitor_report.send_drift_email_alert(DRIFT_EMAIL_RECEIVER, send_for = 'model_drift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth - Day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the production data (i.e. new data that our model is expected to classify in production)\n",
    "# Note: Data has been stored by Ioannis M. for demonstrating purposes\n",
    "df_truth_day1 = pd.read_pickle('./src/data/assets/df_day1_ground_truth.pickle')\n",
    "df_truth_day1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction on the training data (just for demonstrating purposes)\n",
    "df_truth_day1['prediction'] = lasso_model.predict(df_truth_day1.drop('wine_colour', axis=1))\n",
    "df_truth_day1['prediction'] = df_truth_day1['prediction'].map({1: 'white', 0: 'red'})\n",
    "df_truth_day1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the prediction for the ground truth data\n",
    "df_reference = df.copy()\n",
    "\n",
    "df_reference['prediction'] = lasso_model.predict(df_reference.drop('wine_colour', axis=1)) # Drop the response temporarily to predict\n",
    "df_reference['prediction'] = df_reference['prediction'].map({1: 'white', 0: 'red'})\n",
    "df_reference.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the global column_mapping\n",
    "column_mapping = ast.literal_eval(config['model_monitor']['column_mapping'])\n",
    "column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model monitor report and generate the data & model dashboard\n",
    "# for the ground truth day 1\n",
    "model_monitor_reports = ModelMonitorReports(df_reference, df_truth_day1, column_mapping)\n",
    "model_monitor_reports.generate_model_data_drift_report(response_type = 'categorical', report_name = 'poc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MonitorReportReader class\n",
    "monitor_report = MonitorReportReader(LATEST_DRIFT_REPORT_PATH, PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the data drift\n",
    "monitor_report.create_model_drift_table()\n",
    "monitor_report.df_target_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send alert for model_drift\n",
    "monitor_report.send_drift_email_alert(DRIFT_EMAIL_RECEIVER, send_for = 'model_drift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Production - Day 2 (Drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the production data (i.e. new data that our model is expected to classify in production)\n",
    "# Note: Data has been stored by Ioannis M. for demonstrating purposes\n",
    "df_production_day2 = pd.read_pickle('src/data/assets/df_day2.pickle')\n",
    "df_production_day2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the prediction on the training data (just for demonstrating purposes)\n",
    "df_production_day2['prediction'] = lasso_model.predict(df_production_day2)\n",
    "df_production_day2['prediction'] = df_production_day2['prediction'].map({1: 'white', 0: 'red'})\n",
    "df_production_day2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore how the reference data looks\n",
    "df_reference.drop('wine_colour', axis=1, inplace=True)\n",
    "df_reference.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the column mapping\n",
    "column_mapping['target'] = None\n",
    "column_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model monitor report and generate the data & model dashboard\n",
    "# for the production day2\n",
    "model_monitor_reports = ModelMonitorReports(df_reference, df_production_day2, column_mapping)\n",
    "model_monitor_reports.generate_model_data_drift_report(response_type = 'categorical', report_name = 'poc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MonitorReportReader class\n",
    "monitor_report = MonitorReportReader(LATEST_DRIFT_REPORT_PATH, PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model drift dataframe\n",
    "monitor_report.create_model_drift_table(response_type = 'categorical', p_value_threshold = 1)\n",
    "monitor_report.df_target_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an automated e-mail to capture data drift\n",
    "monitor_report.send_drift_email_alert(DRIFT_EMAIL_RECEIVER, send_for = 'model_drift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/evidentlyai/evidently/blob/main/evidently/examples/bicycle_demand_monitoring.ipynb\n",
    "# https://evidentlyai.com/blog/tutorial-1-model-analytics-in-production\n",
    "# https://docs.evidentlyai.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
